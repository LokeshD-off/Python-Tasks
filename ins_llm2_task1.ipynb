{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tehnoidentity/Desktop/Python_Tasks/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /home/tehnoidentity/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 7.33 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7500.85 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 16000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size = 12500.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 12500.00 MiB, K (f16): 6250.00 MiB, V (f16): 6250.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  1321.26 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =    5408.57 ms\n",
      "llama_print_timings:      sample time =     135.15 ms /   200 runs   (    0.68 ms per token,  1479.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5395.66 ms /    14 tokens (  385.40 ms per token,     2.59 tokens per second)\n",
      "llama_print_timings:        eval time =  147677.30 ms /   199 runs   (  742.10 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time =  153509.77 ms /   213 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are many successful businessmen who have earned a significant amount of money throughout their careers. However, some of the most successful ones in terms of earned money include:\n",
      "\n",
      "1. Bill Gates - Co-founder of Microsoft, Gates is one of the richest people in the world with an estimated net worth of over $200 billion. He has earned his wealth primarily through his ownership of Microsoft stock and his various business ventures.\n",
      "\n",
      "2. Warren Buffett - Known as the \"Oracle of Omaha,\" Buffett is a successful investor and CEO of Berkshire Hathaway. He has an estimated net worth of over $100 billion and has earned his wealth through his savvy investments and business acumen.\n",
      "\n",
      "3. Jeff Bezos - Founder and CEO of Amazon, Bezos has an estimated net worth of over $200 billion. He has earned his wealth primarily through\n"
     ]
    }
   ],
   "source": [
    "##Imports\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "## Download the GGUF model\n",
    "model_name = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
    "model_file = \"llama-2-13b-chat.Q4_K_M.gguf\" \n",
    "model_path = hf_hub_download(model_name, filename=model_file)\n",
    "\n",
    "## Instantiate model from downloaded file\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=16000,  # Context length to use\n",
    "    n_threads=32,            # Number of CPU threads to use\n",
    "    n_gpu_layers=0        # Number of model layers to offload to GPU\n",
    ")\n",
    "\n",
    "## Generation kwargs\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\":200,\n",
    "    \"stop\":[\"</s>\"],\n",
    "    \"echo\":False, # Echo the prompt in the output\n",
    "    \"top_k\":1,# This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
    "    \"temperature\" : 0\n",
    "}\n",
    "\n",
    "## Run inference\n",
    "prompt = \"Who is the most successful business man in terms of earned money?\"\n",
    "res = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
    "\n",
    "## Unpack and the generated text from the LLM response dictionary and print it\n",
    "print(res[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5408.57 ms\n",
      "llama_print_timings:      sample time =     128.01 ms /   200 runs   (    0.64 ms per token,  1562.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =  144567.14 ms /   200 runs   (  722.84 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:       total time =  144950.95 ms /   200 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are many successful businessmen who have earned a significant amount of money through their ventures. However, it's difficult to determine who the most successful one is, as there are various factors to consider such as the industry they operate in, the size of their companies, and the time period over which they have been successful.\n",
      "\n",
      "That being said, some of the most successful businessmen in terms of earned money include:\n",
      "\n",
      "1. Jeff Bezos - Founder and CEO of Amazon, with an estimated net worth of over $200 billion.\n",
      "2. Bill Gates - Co-founder of Microsoft, with an estimated net worth of over $150 billion.\n",
      "3. Warren Buffett - Investor and CEO of Berkshire Hathaway, with an estimated net worth of over $90 billion.\n",
      "4. Mark Zuckerberg - Co-founder and CEO of Facebook, with an estimated net worth\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs_2 = {\n",
    "    \"max_tokens\" : 200,\n",
    "    \"stop\" : [\"</s>\"],\n",
    "    \"echo\" : False,\n",
    "    \"top_k\" : 1,\n",
    "    \"temperature\" : 1\n",
    "}\n",
    "\n",
    "#prompt\n",
    "prompt_2 = \"Who is the most successful business man in terms of earned money?\"\n",
    "response_2 = llm(prompt_2,**generation_kwargs_2)\n",
    "\n",
    "#printing the response\n",
    "print(response_2[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5408.57 ms\n",
      "llama_print_timings:      sample time =       4.77 ms /     7 runs   (    0.68 ms per token,  1468.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6132.51 ms /    32 tokens (  191.64 ms per token,     5.22 tokens per second)\n",
      "llama_print_timings:        eval time =    4286.02 ms /     6 runs   (  714.34 ms per token,     1.40 tokens per second)\n",
      "llama_print_timings:       total time =   10429.14 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: Three.\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs_3 = {\n",
    "    \"max_tokens\" : 200,\n",
    "    \"stop\" : [\"</s>\"],\n",
    "    \"echo\" : False,\n",
    "    \"top_k\" : 1,\n",
    "    \"temperature\" : 1\n",
    "}\n",
    "\n",
    "prompt_3 = \"There are two killers in a room, if another guy enters the room and kills one, now how many killers are there in the room?\"\n",
    "response_3 = llm(prompt_3,**generation_kwargs_3)\n",
    "\n",
    "print(response_3[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5408.57 ms\n",
      "llama_print_timings:      sample time =       4.91 ms /     7 runs   (    0.70 ms per token,  1425.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    5222.45 ms /     7 runs   (  746.06 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time =    5233.30 ms /     7 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: Three.\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs_3 = {\n",
    "    \"max_tokens\" : 200,\n",
    "    \"stop\" : [\"</s>\"],\n",
    "    \"echo\" : False,\n",
    "    \"top_k\" : 1,\n",
    "    \"temperature\" : 0\n",
    "}\n",
    "\n",
    "prompt_3 = \"There are two killers in a room, if another guy enters the room and kills one, now how many killers are there in the room?\"\n",
    "response_3 = llm(prompt_3,**generation_kwargs_3)\n",
    "\n",
    "print(response_3[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5408.57 ms\n",
      "llama_print_timings:      sample time =      11.97 ms /    19 runs   (    0.63 ms per token,  1586.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1667.03 ms /     5 tokens (  333.41 ms per token,     3.00 tokens per second)\n",
      "llama_print_timings:        eval time =   12874.49 ms /    18 runs   (  715.25 ms per token,     1.40 tokens per second)\n",
      "llama_print_timings:       total time =   14570.99 ms /    23 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: One. The person who entered the room is also a killer.\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs_3 = {\n",
    "    \"max_tokens\" : 200,\n",
    "    \"stop\" : [\"</s>\"],\n",
    "    \"echo\" : False,\n",
    "    \"top_k\" : 1,\n",
    "    \"temperature\" : 1\n",
    "}\n",
    "\n",
    "prompt_3 = \"There are two killers in a room, if another guy enters the room and kills one, now how many killers are alive in the room?\"\n",
    "response_3 = llm(prompt_3,**generation_kwargs_3)\n",
    "\n",
    "print(response_3[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5408.57 ms\n",
      "llama_print_timings:      sample time =      11.69 ms /    19 runs   (    0.62 ms per token,  1624.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   13712.99 ms /    19 runs   (  721.74 ms per token,     1.39 tokens per second)\n",
      "llama_print_timings:       total time =   13738.14 ms /    19 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: One. The person who entered the room is also a killer.\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs_3 = {\n",
    "    \"max_tokens\" : 200,\n",
    "    \"stop\" : [\"</s>\"],\n",
    "    \"echo\" : False,\n",
    "    \"top_k\" : 1,\n",
    "    \"temperature\" : 0\n",
    "}\n",
    "\n",
    "prompt_3 = \"There are two killers in a room, if another guy enters the room and kills one, now how many killers are alive in the room?\"\n",
    "response_3 = llm(prompt_3,**generation_kwargs_3)\n",
    "\n",
    "print(response_3[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
